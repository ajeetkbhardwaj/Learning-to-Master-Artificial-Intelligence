{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-80 : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Modeling\n",
    "\n",
    "Generative Modeling by estimating gradient of the data distribution i.e We learn score functions(gradient of the log probability density function) on a large number of noisy purturbed data distributions and then we generate samples with Langevin-type sampling is called score based generative model. \n",
    "\n",
    "Score based generative models have several advantages over existing generative model families like  GAN-level sample quality without adversarial training, flexible model architectures, exact log-likelihood computation, and inverse problem solving without re-training models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How generative models represent probability distributions ? \n",
    "\n",
    "The generative modeling techniques can be classified into two types based on above question\n",
    "1. Likelihood-based Models learn the distribution's probability density function by approximation of maximum likelihood. Autoregressive models,Normalizing flow models, Energy-based models (EBMs), and Variational auto-encoders (VAEs).\n",
    "2. Implicit Generative Models probability distribution is implicitly represented by a model of its sampling process. Generative Adverserial Networks(GANs), new smples from data distribution are synthesized by transforming a random gaussian vector with a neural network."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

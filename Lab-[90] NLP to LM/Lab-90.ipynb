{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab-90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is Regular Expression ?\n",
    "> A set of characters is called as pattern, which helphs in finding substrings in a given string. Thus pattern is used to detect the substrings in a given string. Hence We can use regular expression for text processing tasks like text cleaning and handling our text in a much better way.\\\n",
    "> Example-1 : Customers review dataset about your services \\\n",
    "> Example-2 : Artificial Assistants like Google, Siri "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)\n",
    "![alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are Fundamental NLP Tasks ?\n",
    "\n",
    "1. Tokenization : Given a sentence, segment it's sequence of characters into tokens. Let's take an example **Input** : I am going to study NLP today. **Expected Output**: I, am , going, to, study, NLP, Today.\n",
    "\n",
    "2. Stop-ward removal : It's all about removing common words. Let's take an example **Input** : I am going to study NLP today. **Expected Output** : going, study, NLP, today.\n",
    "\n",
    "3. Lemmatization : It's used to reduce the tokens to it's basic forms. Let's take an example, **Input** : going, study, NLP, today. **Expected Output** : go, study, NLP, today.\n",
    "\n",
    "4. Part of Speech(PoS) tagging : It's used to assign each token a particular part of speech tag, based on both its definition and context. Let's take an example, **Input** : going, study, NLP, today. **Expected Output** : Verb, Adverb, Noun, Noun.\n",
    "\n",
    "5. Named Entity Extraction : We Identify the named entities like Person, Country, organization, etc in a given input sentence. Let's take an example, **Input** : I love Utter Pradesh and its culture. **Expected Output** : India in Input sentence is labeled with Country\n",
    "\n",
    "6. Parse Tree Generation : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab-90 : \n",
    "1. Dividing Docs text into sentences\n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Dividing Docs text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To Sherlock Holmes she is always _the_ woman. I have seldom heard him\n",
      "mention her under any other name. In his eyes she eclipses and\n",
      "predominates the whole of her sex. It was not that he felt any emotion\n",
      "akin to love for Irene Adler. All emotions, and that one particularly,\n",
      "were abhorrent to his cold, precise but admirably balanced mind. He\n",
      "was, I take it, the most perfect reasoning and observing machine that\n",
      "the world has seen, but as a lover he would have placed himself in a\n",
      "false position. He never spoke of the softer passions, save with a gibe\n",
      "and a sneer. They were admirable things for the observer—excellent for\n",
      "drawing the veil from men’s motives and actions. But for the trained\n",
      "reasoner to admit such intrusions into his own delicate and finely\n",
      "adjusted temperament was to introduce a distracting factor which might\n",
      "throw a doubt upon all his mental results. Grit in a sensitive\n",
      "instrument, or a crack in one of his own high-power lenses, would not\n",
      "be more disturbing than a strong emotion in a nature such as his. And\n",
      "yet there was but one woman to him, and that woman was the late Irene\n",
      "Adler, of dubious and questionable memory.\n",
      "\n",
      "\n",
      "When to Use ML At All.\n",
      "ML projects have a higher failure rate than software projects in general. One reason that's worth acknowledging is that for many applications, ML is fundamentally still research. Therefore, we shouldn't aim for 100% success.\n",
      "Additionally, many ML projects are doomed to fail even before they are undertaken due to a variety of reasons:\n",
      "They are technically infeasible or poorly scoped.\n",
      "They never make the leap to a production environment.\n",
      "The broader organization is not all on the same page about what would be considered success criteria for them.\n",
      "They solve the problem that you set out to solve but do not solve a big enough problem to be worth their complexity.\n",
      "The bar for your ML projects should be that their value must outweigh not just the cost of developing them but also the additional complexity that these ML systems introduce to your software (as introduced in the classic paper \"The High-Interest Credit Card of Technical Debt\").\n",
      "In brief, ML systems erode the boundaries between other systems, rely on expensive data dependencies, are commonly plagued by system design anti-patterns, and are subject to the instability of the external world.\n",
      "Before starting an ML project, ask yourself:\n",
      "Are you ready to use ML? More specifically, do you have a product? Are you collecting data and storing it in a sane way? Do you have the right people?\n",
      "Do you really need ML to solve this problem? More specifically, do you need to solve the problem at all? Have you tried using rules or simple statistics to solve the problem?\n",
      "Is it ethical to use ML to solve this problem? We have a whole lecture about ethics!\n",
      "How to Pick Problems to Solve with ML\n",
      "Just like any other project prioritization, you want to look for use cases that have high impact and low cost:\n",
      "High-impact problems are likely to be those that address friction in your product, complex parts of your pipeline, places where cheap prediction is valuable, and generally what other people in your industry are doing.\n",
      "Low-cost projects are those with available data, where bad predictions are not too harmful.\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "URLError",
     "evalue": "<urlopen error unknown url type: c>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(sherlock_holmes)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# tokenizer loading\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/sony.gosala/AppData/Roaming/nltk_data/tokenizers/punkt/english.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Word into sentences\u001b[39;00m\n\u001b[0;32m     15\u001b[0m sentences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sherlock_holmes)\n",
      "File \u001b[1;32mc:\\Users\\sony.gosala\\.conda\\envs\\SciComp\\Lib\\site-packages\\nltk\\data.py:836\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 836\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    839\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\sony.gosala\\.conda\\envs\\SciComp\\Lib\\site-packages\\nltk\\data.py:967\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 967\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m urlopen(resource_url)\n",
      "File \u001b[1;32mc:\\Users\\sony.gosala\\.conda\\envs\\SciComp\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opener\u001b[38;5;241m.\u001b[39mopen(url, data, timeout)\n",
      "File \u001b[1;32mc:\\Users\\sony.gosala\\.conda\\envs\\SciComp\\Lib\\urllib\\request.py:515\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    512\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    514\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 515\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_open(req, data)\n\u001b[0;32m    517\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    518\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sony.gosala\\.conda\\envs\\SciComp\\Lib\\urllib\\request.py:537\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_open, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    538\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown_open\u001b[39m\u001b[38;5;124m'\u001b[39m, req)\n",
      "File \u001b[1;32mc:\\Users\\sony.gosala\\.conda\\envs\\SciComp\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\sony.gosala\\.conda\\envs\\SciComp\\Lib\\urllib\\request.py:1420\u001b[0m, in \u001b[0;36mUnknownHandler.unknown_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munknown_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m-> 1420\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m URLError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown url type: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m)\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error unknown url type: c>"
     ]
    }
   ],
   "source": [
    "# 1.1 dependencies\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "%run -i \"./utils/file_utils.ipynb\"\n",
    "\n",
    "# load the text docs\n",
    "sherlock_holmes = read_text_file('./data/sherlock_holmes.txt')\n",
    "print(sherlock_holmes)\n",
    "\n",
    "# tokenizer loading\n",
    "tokenizer = nltk.data.load('C:/Users/sony.gosala/AppData/Roaming/nltk_data/tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Word into sentences\n",
    "sentences = tokenizer.tokenize(sherlock_holmes)\n",
    "print(sentences)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "Labs : \n",
    "1. [Fundamental NLP Tasks like Tokenization, Stop-word removal, lemmatization, part of speech etc](https://github.com/PacktPublishing/Python-Natural-Language-Processing-Cookbook-Second-Edition/tree/main/Chapter01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciComp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab-95 : Text Data Preprocessing\n",
        "> What we will going to do ?, convert text data into the vectors or tensors via text encoding(text embedding) techniques like OneHote Encoding, Word2Vec, Bag of Words, (BoW), N-grams TF-IDR and AvgWord2Vec etc."
      ],
      "metadata": {
        "id": "7MuRlg_Ocp0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step-00 : Setup the dependencies\n"
      ],
      "metadata": {
        "id": "xdDHeGmJdn66"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "DBtgUf1PcjAw",
        "outputId": "242d8876-1408-4f93-ad2c-535a4cfb5469",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import nltk"
      ],
      "metadata": {
        "id": "zB5eeI9Yen6W"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "fn92Y_fSfIDR",
        "outputId": "550e4d28-8b70-4580-8d11-b9a5f0e2f9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-01 : Tokenization"
      ],
      "metadata": {
        "id": "SEcEMlK8g-9i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hR-YeXaOcjAx",
        "outputId": "d5692474-755a-4b22-9fca-df73b2970b22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "What we will going to do ?, convert text data into the vectors or \n",
            "tensors via text encoding(text embedding) techniques like OneHote Encoding, Word2Vec, \n",
            "Bag of Words, (BoW), N-grams TF-IDR and AvgWord2Vec etc. What a wonderfull technique !.\n",
            "\n",
            "<class 'list'>\n",
            "\n",
            "What we will going to do ?, convert text data into the vectors or \n",
            "tensors via text encoding(text embedding) techniques like OneHote Encoding, Word2Vec, \n",
            "Bag of Words, (BoW), N-grams TF-IDR and AvgWord2Vec etc.\n",
            "What a wonderfull technique !.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What',\n",
              " 'we',\n",
              " 'will',\n",
              " 'going',\n",
              " 'to',\n",
              " 'do',\n",
              " '?',\n",
              " ',',\n",
              " 'convert',\n",
              " 'text',\n",
              " 'data',\n",
              " 'into',\n",
              " 'the',\n",
              " 'vectors',\n",
              " 'or',\n",
              " 'tensors',\n",
              " 'via',\n",
              " 'text',\n",
              " 'encoding',\n",
              " '(',\n",
              " 'text',\n",
              " 'embedding',\n",
              " ')',\n",
              " 'techniques',\n",
              " 'like',\n",
              " 'OneHote',\n",
              " 'Encoding',\n",
              " ',',\n",
              " 'Word2Vec',\n",
              " ',',\n",
              " 'Bag',\n",
              " 'of',\n",
              " 'Words',\n",
              " ',',\n",
              " '(',\n",
              " 'BoW',\n",
              " ')',\n",
              " ',',\n",
              " 'N-grams',\n",
              " 'TF-IDR',\n",
              " 'and',\n",
              " 'AvgWord2Vec',\n",
              " 'etc',\n",
              " '.',\n",
              " 'What',\n",
              " 'a',\n",
              " 'wonderfull',\n",
              " 'technique',\n",
              " '!',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "text = \"\"\"\n",
        "What we will going to do ?, convert text data into the vectors or\n",
        "tensors via text encoding(text embedding) techniques like OneHote Encoding, Word2Vec,\n",
        "Bag of Words, (BoW), N-grams TF-IDR and AvgWord2Vec etc. What a wonderfull technique !.\n",
        "\"\"\"\n",
        "print(text)\n",
        "\n",
        "# 1. tokenization : sentences to paragraphs\n",
        "docs = sent_tokenize(text)\n",
        "print(type(docs))\n",
        "\n",
        "for sentence in docs:\n",
        "  print(sentence)\n",
        "# 2. tokenization : paragraph, sentences to words\n",
        "word_tokenize(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in docs:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "id": "UgpG1G6Geb3o",
        "outputId": "a72aecdb-ff4c-4f23-b08a-23550aaedb5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'we', 'will', 'going', 'to', 'do', '?', ',', 'convert', 'text', 'data', 'into', 'the', 'vectors', 'or', 'tensors', 'via', 'text', 'encoding', '(', 'text', 'embedding', ')', 'techniques', 'like', 'OneHote', 'Encoding', ',', 'Word2Vec', ',', 'Bag', 'of', 'Words', ',', '(', 'BoW', ')', ',', 'N-grams', 'TF-IDR', 'and', 'AvgWord2Vec', 'etc', '.']\n",
            "['What', 'a', 'wonderfull', 'technique', '!', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(text)"
      ],
      "metadata": {
        "id": "RLk8XgWvgFIe",
        "outputId": "8d566a85-d987-4c4d-e903-fdfcba6fa641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What',\n",
              " 'we',\n",
              " 'will',\n",
              " 'going',\n",
              " 'to',\n",
              " 'do',\n",
              " '?,',\n",
              " 'convert',\n",
              " 'text',\n",
              " 'data',\n",
              " 'into',\n",
              " 'the',\n",
              " 'vectors',\n",
              " 'or',\n",
              " 'tensors',\n",
              " 'via',\n",
              " 'text',\n",
              " 'encoding',\n",
              " '(',\n",
              " 'text',\n",
              " 'embedding',\n",
              " ')',\n",
              " 'techniques',\n",
              " 'like',\n",
              " 'OneHote',\n",
              " 'Encoding',\n",
              " ',',\n",
              " 'Word2Vec',\n",
              " ',',\n",
              " 'Bag',\n",
              " 'of',\n",
              " 'Words',\n",
              " ',',\n",
              " '(',\n",
              " 'BoW',\n",
              " '),',\n",
              " 'N',\n",
              " '-',\n",
              " 'grams',\n",
              " 'TF',\n",
              " '-',\n",
              " 'IDR',\n",
              " 'and',\n",
              " 'AvgWord2Vec',\n",
              " 'etc',\n",
              " '.',\n",
              " 'What',\n",
              " 'a',\n",
              " 'wonderfull',\n",
              " 'technique',\n",
              " '!.']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize((text))"
      ],
      "metadata": {
        "id": "TKhdLBuBgb9V",
        "outputId": "7d888bb9-3971-4260-f728-e0f5d6bc80c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What',\n",
              " 'we',\n",
              " 'will',\n",
              " 'going',\n",
              " 'to',\n",
              " 'do',\n",
              " '?',\n",
              " ',',\n",
              " 'convert',\n",
              " 'text',\n",
              " 'data',\n",
              " 'into',\n",
              " 'the',\n",
              " 'vectors',\n",
              " 'or',\n",
              " 'tensors',\n",
              " 'via',\n",
              " 'text',\n",
              " 'encoding',\n",
              " '(',\n",
              " 'text',\n",
              " 'embedding',\n",
              " ')',\n",
              " 'techniques',\n",
              " 'like',\n",
              " 'OneHote',\n",
              " 'Encoding',\n",
              " ',',\n",
              " 'Word2Vec',\n",
              " ',',\n",
              " 'Bag',\n",
              " 'of',\n",
              " 'Words',\n",
              " ',',\n",
              " '(',\n",
              " 'BoW',\n",
              " ')',\n",
              " ',',\n",
              " 'N-grams',\n",
              " 'TF-IDR',\n",
              " 'and',\n",
              " 'AvgWord2Vec',\n",
              " 'etc.',\n",
              " 'What',\n",
              " 'a',\n",
              " 'wonderfull',\n",
              " 'technique',\n",
              " '!',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-02 : Stemming, RegexpStemmer class, Snowball Stemmer\n",
        "> A process of reducing a word to its word stem that affixes to suffixes and prefixes or to root of words known as lemma. Thus Stemming is usefull in Natural Language Understanding and Processing.\n",
        "\n",
        "> RegexpStemmer is a class in nltk used to implement Regular expression stemmer algorithms. It takes a single regular expression and removes any prefix or suffix that matches the expression.\n",
        "\n",
        "> Snowball Stemmer also a stemming algorithm know as Porter2 stemming algorithm."
      ],
      "metadata": {
        "id": "N3x5yDuVhO3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer,  SnowballStemmer"
      ],
      "metadata": {
        "id": "_bjxo64WiFZk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Classification Problem is to classify the reviews of product as positive or negative review\n",
        "words = [\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n",
        "stemming = PorterStemmer()\n",
        "for word in words:\n",
        "  print(word+\"--->\"+stemming.stem(word))\n",
        "\n",
        "print(stemming.stem('congratulations'))\n",
        "print(stemming.stem('sitting'))"
      ],
      "metadata": {
        "id": "N5a2X9x_g71-",
        "outputId": "569342ea-ffca-4484-e45b-dd6d8dbb7951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating--->eat\n",
            "eats--->eat\n",
            "eaten--->eaten\n",
            "writing--->write\n",
            "writes--->write\n",
            "programming--->program\n",
            "programs--->program\n",
            "history--->histori\n",
            "finally--->final\n",
            "finalized--->final\n",
            "congratul\n",
            "sit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowStemmer = SnowballStemmer('english')\n",
        "for word in words:\n",
        "  print(word+\"--->\"+snowStemmer.stem(word))"
      ],
      "metadata": {
        "id": "PzrE5H8tiS3y",
        "outputId": "fd60e7b2-540c-427a-b2ec-1279013bf1a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating--->eat\n",
            "eats--->eat\n",
            "eaten--->eaten\n",
            "writing--->write\n",
            "writes--->write\n",
            "programming--->program\n",
            "programs--->program\n",
            "history--->histori\n",
            "finally--->final\n",
            "finalized--->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('fairly'), stemming.stem('sportlighingly')"
      ],
      "metadata": {
        "id": "k7yi3BiMj4O6",
        "outputId": "43081745-6260-4026-9b49-b84f05f17991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportlighingli')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowStemmer.stem('fairly'), snowStemmer.stem('sportlightingly')"
      ],
      "metadata": {
        "id": "e4Hz1A-LkBqD",
        "outputId": "e2db7651-ce1c-4b31-c043-61d078e820d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sportlight')"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('goes'), snowStemmer.stem('goes')"
      ],
      "metadata": {
        "id": "bC_alC27kPL7",
        "outputId": "bd4f847f-2cfb-4dcb-facb-72d4950a4fb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('goe', 'goe')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W96T36RkkewV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mne",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}